{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import wandb\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import einsum, rearrange, reduce\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "def tokenize(raw_text):\n",
    "  raw_text = raw_text['text'][0]\n",
    "  token = [ord(x) for x in raw_text]\n",
    "  current_token = []\n",
    "  next_token = []\n",
    "  for idx in range(len(token) // seq_len):\n",
    "    t = token[idx:idx + seq_len + 1]\n",
    "    current_token.append(t[:-1])\n",
    "    next_token.append(t[1:])\n",
    "  return {'current': current_token,\n",
    "          'next': next_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_data = datasets.load_dataset('karpathy/tiny_shakespeare', split='train')\n",
    "raw_text_data_val = datasets.load_dataset('karpathy/tiny_shakespeare', split='validation')\n",
    "char_data_train = raw_text_data.map(tokenize, batched=True, remove_columns=['text']).with_format('torch')\n",
    "char_data_val = raw_text_data_val.map(tokenize, batched=True, remove_columns=['text']).with_format('torch')\n",
    "train_dataloader = DataLoader(char_data_train, batch_size=10, shuffle=True)\n",
    "val_dataloader = DataLoader(char_data_val, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, d_model: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.embedding_matrix = nn.Parameter(torch.zeros(256, d_model))\n",
    "    nn.init.xavier_normal_(self.embedding_matrix)\n",
    "\n",
    "    self.positional_encoding = nn.Parameter(torch.zeros(seq_len, d_model))\n",
    "    nn.init.xavier_normal_(self.positional_encoding)\n",
    "\n",
    "  def forward(self, data: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    return self.embedding_matrix[data] + self.positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, n_head: int, d_model: int, d_head: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.seq_len = seq_len\n",
    "    self.d_head = d_head\n",
    "\n",
    "    self.query_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.query_matrix)\n",
    "\n",
    "    self.key_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.key_matrix)\n",
    "\n",
    "    self.value_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.value_matrix)\n",
    "\n",
    "    self.output_matrix = nn.Parameter(torch.zeros(n_head, d_model, d_head))\n",
    "    nn.init.xavier_normal_(self.output_matrix)\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    query = einsum(data, self.query_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "    key = einsum(data, self.key_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "    value = einsum(data, self.value_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "\n",
    "    attn_pre = einsum(query, key, \"batch n_head query_len d_head, batch n_head key_len d_head -> batch n_head query_len key_len\")\n",
    "    mask_idx = torch.triu_indices(self.seq_len, self.seq_len, offset=1)\n",
    "    attn_pre[..., mask_idx[0], mask_idx[1]] = float('-inf')\n",
    "    attn_pre /= self.d_head ** 0.5\n",
    "    attn = F.softmax(attn_pre, dim=-1)\n",
    "\n",
    "    output_pre = einsum(attn, value, \"batch n_head query_len key_len, batch n_head key_len d_head -> batch n_head key_len d_head\")\n",
    "    output = einsum(self.output_matrix, output_pre, \"n_head d_model d_head, batch n_head seq_len d_head -> batch seq_len d_model\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, d_model: int, d_mlp: int):\n",
    "    super().__init__()\n",
    "    self.MLP = nn.Sequential(nn.Linear(d_model, d_mlp),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(d_mlp, d_model))\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    return self.MLP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]):\n",
    "    mean: Float[Tensor, \"batch\"] = data.mean(dim=[1, 2], keepdim=True)\n",
    "    std: Float[Tensor, \"batch\"] = data.std(dim=[1, 2], keepdim=True)\n",
    "    return (data - mean) / (std + 1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembedding(nn.Module):\n",
    "  def __init__(self, d_model: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.unembedding_matrix = nn.Parameter(torch.zeros(d_model, 256))\n",
    "    nn.init.xavier_normal_(self.unembedding_matrix)\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len 256\"]:\n",
    "    return einsum(self.unembedding_matrix, data, \"d_model d_vocab, batch seq_len d_model -> batch seq_len d_vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, n_heads: int, d_model: int, d_head: int, seq_len: int, d_mlp: int ):\n",
    "    super().__init__()\n",
    "    self.Attn = Attention(n_heads, d_model, d_head, seq_len)\n",
    "    self.MLP = MLP(d_model, d_mlp)\n",
    "    self.LayerNorm = LayerNorm()\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    resid_attn = self.LayerNorm(data + self.Attn(data))\n",
    "    return self.LayerNorm(resid_attn + self.MLP(resid_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "  d_model: int\n",
    "  ctx_len: int\n",
    "  n_heads: int\n",
    "  d_head: int\n",
    "  d_mlp: int\n",
    "  n_layers: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "  def __init__(self, model_cfg: TransformerConfig):\n",
    "    super().__init__()\n",
    "    self.Embed = Embedding(model_cfg.d_model, model_cfg.ctx_len)   \n",
    "    self.Unembed = Unembedding(model_cfg.d_model, model_cfg.ctx_len)\n",
    "    self.Layers = nn.ModuleList([TransformerLayer(model_cfg.n_heads, model_cfg.d_model, model_cfg.d_head, model_cfg.ctx_len, model_cfg.d_mlp) for _ in range(model_cfg.n_layers)])\n",
    "\n",
    "  def forward(self, data: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len 256\"]:\n",
    "    x = self.Embed(data)\n",
    "    for tl in self.Layers: \n",
    "      x = tl(x)\n",
    "    return self.Unembed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(768, 512, 12, 64, 3072, 3)\n",
    "gpt2 = GPT2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(torch.randint(256, (1, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(gpt2.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "  for data in tqdm(train_dataloader, leave=False):\n",
    "    current_tok = data['current']\n",
    "    next_tok = data['next']\n",
    "\n",
    "    logits = gpt2(current_tok)\n",
    "    logits = rearrange(logits, \"batch ctx_len d_vocab -> batch d_vocab ctx_len\")\n",
    "    loss = loss_fn(logits, next_tok)\n",
    "    wandb.log({'loss': loss.item()})\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rwzh0viv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.28882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-star-24</strong> at: <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/rwzh0viv' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/rwzh0viv</a><br/> View project at: <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_160548-rwzh0viv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rwzh0viv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/femibello/Documents/projects/gpt2-rep/wandb/run-20241115_161311-0t1uxtxz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/0t1uxtxz' target=\"_blank\">leafy-violet-25</a></strong> to <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/0t1uxtxz' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/0t1uxtxz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c4a619a71a44c48b87f752cdb74efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ca25bcf20646d19dbb4f6242326638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"gpt2-rep\")\n",
    "for _ in tqdm(range(10)):\n",
    "  train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.Tensor([[ord(x) for x in \"\"\"No, no, it cannot be; and yet my heart\n",
    "\"\"\"]]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 39])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, no, it cannot be; and yet my heart\n",
      "it:\n",
      "Vesovesot o a\u0019endirthelknt nventhe k I.\n",
      "isento moneng fen: fo ct haceme\n",
      "\n",
      "\n",
      "piaio thed coutd tinelsaspry ty bun:\n",
      "\n",
      "Velliuseld, yostit@s Cidizerind musthalit renthawen:\n",
      "Wobe bothatheoun:\n",
      "ullicogat bent or tut aluseculue.itr ui way, nf n mourtheans he an.\n",
      "Wentould at izelases hary.\n",
      "\n",
      "\n",
      "Whuthonve's hito\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Auto-regressive sampling with padding handling\n",
    "def sample(model, tokens, n_tokens, pad_token_id=0):\n",
    "    # Pad the initial tokens to 512 tokens\n",
    "    non_padded_len = tokens.size(1)\n",
    "    pad_length = 512 - tokens.size(1)\n",
    "    if pad_length > 0:\n",
    "        tokens = torch.cat([tokens, torch.full((tokens.size(0), pad_length), pad_token_id, dtype=tokens.dtype)], dim=1)\n",
    "    \n",
    "    current_length = non_padded_len\n",
    "    for _ in range(n_tokens):\n",
    "        logits = model(tokens)\n",
    "        logits = logits[0, current_length - 1]\n",
    "        # Sample the next token probabilistically\n",
    "        token = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "        # replace the last token with the sampled token\n",
    "        tokens[0, current_length] = token\n",
    "        current_length += 1\n",
    "    return tokens\n",
    "\n",
    "# Convert the token back to string\n",
    "def to_string(tokens):\n",
    "    return ''.join([chr(x) for x in tokens[0]])\n",
    "\n",
    "sampled_tokens = sample(gpt2, tokens, 300)\n",
    "print(to_string(sampled_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
