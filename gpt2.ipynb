{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from einops import einsum, rearrange, reduce\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "def tokenize(raw_text):\n",
    "  raw_text = raw_text['text'][0]\n",
    "  token = [ord(x) for x in raw_text]\n",
    "  current_token = []\n",
    "  next_token = []\n",
    "  for idx in range(len(token) // seq_len):\n",
    "    t = token[idx:idx + seq_len + 1]\n",
    "    current_token.append(t[:-1])\n",
    "    next_token.append(t[1:])\n",
    "  return {'current': current_token,\n",
    "          'next': next_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_data = datasets.load_dataset('karpathy/tiny_shakespeare', split='train')\n",
    "char_data = raw_text_data.map(tokenize, batched=True, remove_columns=['text']).with_format('torch')\n",
    "train_dataloader = DataLoader(char_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, d_model: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.embedding_matrix = nn.Parameter(torch.zeros(256, d_model))\n",
    "    nn.init.xavier_normal_(self.embedding_matrix)\n",
    "\n",
    "    self.positional_encoding = nn.Parameter(torch.zeros(seq_len, d_model))\n",
    "    nn.init.xavier_normal_(self.positional_encoding)\n",
    "\n",
    "  def forward(self, data: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    return self.embedding_matrix[data] + self.positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, n_head: int, d_model: int, d_head: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.seq_len = seq_len\n",
    "    self.d_head = d_head\n",
    "\n",
    "    self.query_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.query_matrix)\n",
    "\n",
    "    self.key_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.key_matrix)\n",
    "\n",
    "    self.value_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.value_matrix)\n",
    "\n",
    "    self.output_matrix = nn.Parameter(torch.zeros(n_head, d_model, d_head))\n",
    "    nn.init.xavier_normal_(self.output_matrix)\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    query = einsum(data, self.query_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "    key = einsum(data, self.key_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "    value = einsum(data, self.value_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "\n",
    "    attn_pre = einsum(query, key, \"batch n_head query_len d_head, batch n_head key_len d_head -> batch n_head query_len key_len\")\n",
    "    mask_idx = torch.triu_indices(self.seq_len, self.seq_len, offset=1)\n",
    "    attn_pre[..., mask_idx[0], mask_idx[1]] = float('-inf')\n",
    "    attn_pre /= self.d_head ** 0.5\n",
    "    attn = F.softmax(attn_pre, dim=-1)\n",
    "\n",
    "    output_pre = einsum(attn, value, \"batch n_head query_len key_len, batch n_head key_len d_head -> batch n_head key_len d_head\")\n",
    "    output = einsum(self.output_matrix, output_pre, \"n_head d_model d_head, batch n_head seq_len d_head -> batch seq_len d_model\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, d_model: int, d_mlp: int):\n",
    "    super().__init__()\n",
    "    self.MLP = nn.Sequential(nn.Linear(d_model, d_mlp),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(d_mlp, d_model))\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    return self.MLP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]):\n",
    "    mean: Float[Tensor, \"batch\"] = data.mean(dim=[1, 2], keepdim=True)\n",
    "    std: Float[Tensor, \"batch\"] = data.std(dim=[1, 2], keepdim=True)\n",
    "    return (data - mean) / (std + 1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembedding(nn.Module):\n",
    "  def __init__(self, d_model: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.unembedding_matrix = nn.Parameter(torch.zeros(d_model, 256))\n",
    "    nn.init.xavier_normal_(self.unembedding_matrix)\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len 256\"]:\n",
    "    return einsum(self.unembedding_matrix, data, \"d_model d_vocab, batch seq_len d_model -> batch seq_len d_vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, n_heads: int, d_model: int, d_head: int, seq_len: int, d_mlp: int ):\n",
    "    super().__init__()\n",
    "    self.Attn = Attention(n_heads, d_model, d_head, seq_len)\n",
    "    self.MLP = MLP(d_model, d_mlp)\n",
    "    self.LayerNorm = LayerNorm()\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    resid_attn = self.LayerNorm(data + self.Attn(data))\n",
    "    return self.LayerNorm(resid_attn + self.MLP(resid_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "  d_model: int\n",
    "  ctx_len: int\n",
    "  n_heads: int\n",
    "  d_head: int\n",
    "  d_mlp: int\n",
    "  n_layers: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "  def __init__(self, model_cfg: TransformerConfig):\n",
    "    super().__init__()\n",
    "    self.Embed = Embedding(model_cfg.d_model, model_cfg.ctx_len)   \n",
    "    self.Unembed = Unembedding(model_cfg.d_model, model_cfg.ctx_len)\n",
    "    self.Layers = nn.ModuleList([TransformerLayer(model_cfg.n_heads, model_cfg.d_model, model_cfg.d_head, model_cfg.ctx_len, model_cfg.d_mlp) for _ in range(model_cfg.n_layers)])\n",
    "\n",
    "  def forward(self, data: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len 256\"]:\n",
    "    x = self.Embed(data)\n",
    "    for tl in self.Layers: \n",
    "      x = tl(x)\n",
    "    return self.Unembed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(768, 512, 12, 64, 3072, 5)\n",
    "gpt2 = GPT2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 256])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(torch.randint(256, (1, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(gpt2.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs : int\n",
    "def train_epoch():\n",
    "  global gs\n",
    "  for data in train_dataloader:\n",
    "    current_tok = data['current']\n",
    "    next_tok = data['next']\n",
    "\n",
    "    logits = gpt2(current_tok)\n",
    "    logits = rearrange(logits, \"batch ctx_len d_vocab -> batch d_vocab ctx_len\")\n",
    "    loss = loss_fn(logits, next_tok)\n",
    "    writer.add_scalar('Loss/train', loss, gs)\n",
    "    gs += 1\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474a73c8dd614902a092d831ef77fb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[315], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n",
      "Cell \u001b[0;32mIn[314], line 8\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m current_tok \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m next_tok \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_tok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m logits \u001b[38;5;241m=\u001b[39m rearrange(logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch ctx_len d_vocab -> batch d_vocab ctx_len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, next_tok)\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[310], line 11\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEmbed(data)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayers: \n\u001b[0;32m---> 11\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mtl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUnembed(x)\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[308], line 9\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch seq_len d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch seq_len d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m   resid_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(data \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAttn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(resid_attn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLP(resid_attn))\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[304], line 24\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     21\u001b[0m key \u001b[38;5;241m=\u001b[39m einsum(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_matrix, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m value \u001b[38;5;241m=\u001b[39m einsum(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_matrix, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m attn_pre \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch n_head query_len d_head, batch n_head key_len d_head -> batch n_head query_len key_len\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m mask_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu_indices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m attn_pre[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, mask_idx[\u001b[38;5;241m0\u001b[39m], mask_idx[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/einops/einops.py:907\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    905\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    906\u001b[0m pattern \u001b[38;5;241m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/einops/_backends.py:287\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, pattern, *x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern, \u001b[38;5;241m*\u001b[39mx):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    404\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gs = 0\n",
    "for _ in tqdm(range(10)):\n",
    "  train_epoch()\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3733e-06, 1.6671e-06, 2.1347e-06, 1.6876e-06, 2.5960e-06, 9.7359e-07,\n",
       "        1.7977e-06, 2.1898e-06, 2.1488e-06, 1.6127e-06, 3.2502e-02, 1.4632e-06,\n",
       "        2.1954e-06, 1.5323e-06, 2.0468e-06, 1.3713e-06, 1.7914e-06, 1.4395e-06,\n",
       "        2.1790e-06, 1.7833e-06, 2.5914e-06, 2.3813e-06, 1.9690e-06, 1.2316e-06,\n",
       "        1.8438e-06, 1.8967e-06, 1.4128e-06, 1.0319e-06, 2.3221e-06, 1.9426e-06,\n",
       "        1.7120e-06, 2.5120e-06, 1.5068e-01, 1.7236e-03, 1.9330e-06, 1.5765e-06,\n",
       "        2.0675e-06, 1.5935e-06, 1.4256e-06, 3.1214e-03, 2.2059e-06, 1.7522e-06,\n",
       "        9.4134e-07, 1.2265e-06, 9.7987e-03, 6.9862e-04, 9.3067e-03, 1.5298e-06,\n",
       "        2.3232e-06, 1.4556e-06, 1.6106e-06, 1.6072e-06, 1.5853e-06, 1.3552e-06,\n",
       "        1.9654e-06, 2.9649e-06, 1.8951e-06, 1.6008e-06, 1.1470e-02, 3.7692e-03,\n",
       "        2.5902e-06, 1.4453e-06, 1.2303e-06, 4.2620e-03, 1.7709e-06, 2.7167e-03,\n",
       "        7.6326e-05, 9.0679e-03, 1.5164e-06, 7.8248e-04, 4.0406e-03, 2.1387e-06,\n",
       "        4.9046e-04, 3.3679e-03, 1.5955e-06, 1.7283e-06, 7.9291e-04, 1.5082e-03,\n",
       "        1.6532e-03, 2.4759e-04, 1.3186e-06, 2.3248e-06, 1.1579e-04, 3.4202e-03,\n",
       "        1.2559e-03, 3.9479e-04, 4.3802e-04, 3.7046e-03, 1.7216e-06, 6.0988e-04,\n",
       "        1.9353e-06, 2.5782e-06, 1.8476e-06, 1.3967e-06, 2.1502e-06, 2.1582e-06,\n",
       "        2.4425e-06, 4.2546e-02, 6.3981e-03, 2.4280e-02, 2.6297e-02, 9.4782e-02,\n",
       "        1.2230e-02, 9.6366e-03, 4.3852e-02, 6.6518e-02, 3.9705e-04, 4.8869e-03,\n",
       "        2.3821e-02, 9.3156e-03, 5.2415e-02, 6.1132e-02, 1.3630e-02, 1.7759e-06,\n",
       "        4.3512e-02, 4.8141e-02, 7.6173e-02, 2.7278e-02, 7.7829e-03, 1.5901e-02,\n",
       "        1.6035e-06, 1.9127e-02, 7.5595e-03, 1.5578e-06, 2.5289e-06, 2.2781e-06,\n",
       "        2.4645e-06, 1.5472e-06, 1.4541e-06, 1.8168e-06, 1.5382e-06, 1.6140e-06,\n",
       "        1.5100e-06, 2.4202e-06, 1.9003e-06, 1.4730e-06, 1.5014e-06, 1.5339e-06,\n",
       "        1.5244e-06, 2.4811e-06, 1.8461e-06, 1.9271e-06, 2.2778e-06, 1.9016e-06,\n",
       "        1.4915e-06, 1.9949e-06, 1.9632e-06, 1.7973e-06, 2.1642e-06, 2.4117e-06,\n",
       "        2.2752e-06, 2.9827e-06, 1.1775e-06, 1.9705e-06, 2.1078e-06, 1.6125e-06,\n",
       "        1.4078e-06, 1.5721e-06, 1.8058e-06, 1.9614e-06, 2.1828e-06, 1.3722e-06,\n",
       "        1.7095e-06, 1.9229e-06, 2.1191e-06, 1.8349e-06, 1.6366e-06, 1.9605e-06,\n",
       "        2.3739e-06, 1.7924e-06, 1.0338e-06, 1.2318e-06, 1.7566e-06, 2.0335e-06,\n",
       "        2.0844e-06, 2.0293e-06, 2.6207e-06, 1.8355e-06, 1.5013e-06, 1.8818e-06,\n",
       "        1.7101e-06, 1.5316e-06, 1.5872e-06, 1.5981e-06, 1.9317e-06, 1.7509e-06,\n",
       "        1.2906e-06, 2.1487e-06, 1.8902e-06, 1.1948e-06, 1.8973e-06, 1.6959e-06,\n",
       "        1.3393e-06, 2.0428e-06, 1.7902e-06, 1.4612e-06, 1.6406e-06, 1.8210e-06,\n",
       "        2.1782e-06, 1.7903e-06, 1.9286e-06, 1.4477e-06, 1.5227e-06, 2.8050e-06,\n",
       "        1.7726e-06, 2.1920e-06, 1.9289e-06, 1.4344e-06, 1.4881e-06, 1.8841e-06,\n",
       "        2.2051e-06, 1.3085e-06, 1.3770e-06, 2.4655e-06, 1.6903e-06, 1.9948e-06,\n",
       "        1.5660e-06, 1.8613e-06, 1.7200e-06, 1.1714e-06, 4.2130e-06, 1.7154e-06,\n",
       "        1.5639e-06, 1.6925e-06, 2.3251e-06, 1.8100e-06, 1.5723e-06, 1.3113e-06,\n",
       "        1.2959e-06, 1.2419e-06, 1.3724e-06, 2.6104e-06, 1.6522e-06, 1.3797e-06,\n",
       "        1.5508e-06, 1.3736e-06, 1.6293e-06, 2.2031e-06, 2.4522e-06, 1.7279e-06,\n",
       "        1.2723e-06, 1.7114e-06, 1.3980e-06, 1.7357e-06, 1.5908e-06, 1.4280e-06,\n",
       "        1.9580e-06, 2.0693e-06, 2.0572e-06, 1.4351e-06, 1.5434e-06, 1.4461e-06,\n",
       "        2.1777e-06, 1.9887e-06, 1.1052e-06, 1.5640e-06],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.Tensor([[ord(x) for x in \"\"\"No, no, it cannot be; and yet my heart\n",
    "Will not confess he owes the malady\n",
    "That doth my life besiege. Farewell, young lords;\n",
    "Whether I live or die, be you the sons\n",
    "Of worthy Frenchmen: let higher Italy,--\n",
    "Those bated that inherit but the fall\n",
    "Of the last monarchy,--see that you come\n",
    "Not to woo honour, but to wed it; when\n",
    "The bravest questant shrinks, find what you seek,\n",
    "That fame may cry you loud: I say, farewell.   \n",
    "\n",
    "Why, Doctor She: my lord, there's one arrived,\n",
    "If you will see her: now, by my faith and h\"\"\"]]).long()\n",
    "gpt2(tokens).softmax(dim=-1)[0, -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
