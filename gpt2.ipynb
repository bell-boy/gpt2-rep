{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import wandb\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import einsum, rearrange, reduce\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "def tokenize(raw_text):\n",
    "  raw_text = raw_text['text'][0]\n",
    "  token = [ord(x) for x in raw_text]\n",
    "  current_token = []\n",
    "  next_token = []\n",
    "  for idx in range(len(token) // seq_len):\n",
    "    t = token[idx:idx + seq_len + 1]\n",
    "    current_token.append(t[:-1])\n",
    "    next_token.append(t[1:])\n",
    "  return {'current': current_token,\n",
    "          'next': next_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b962ef1e5b264ca4ac921cceb81efb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_text_data = datasets.load_dataset('karpathy/tiny_shakespeare', split='train')\n",
    "raw_text_data_val = datasets.load_dataset('karpathy/tiny_shakespeare', split='validation')\n",
    "char_data_train = raw_text_data.map(tokenize, batched=True, remove_columns=['text']).with_format('torch')\n",
    "char_data_val = raw_text_data_val.map(tokenize, batched=True, remove_columns=['text']).with_format('torch')\n",
    "train_dataloader = DataLoader(char_data_train, batch_size=10, shuffle=True)\n",
    "val_dataloader = DataLoader(char_data_val, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, d_model: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.embedding_matrix = nn.Parameter(torch.zeros(256, d_model))\n",
    "    nn.init.xavier_normal_(self.embedding_matrix)\n",
    "\n",
    "    self.positional_encoding = nn.Parameter(torch.zeros(seq_len, d_model))\n",
    "    nn.init.xavier_normal_(self.positional_encoding)\n",
    "\n",
    "  def forward(self, data: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    return self.embedding_matrix[data] + self.positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, n_head: int, d_model: int, d_head: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.seq_len = seq_len\n",
    "    self.d_head = d_head\n",
    "\n",
    "    self.query_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.query_matrix)\n",
    "\n",
    "    self.key_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.key_matrix)\n",
    "\n",
    "    self.value_matrix = nn.Parameter(torch.zeros(n_head, d_head, d_model))\n",
    "    nn.init.xavier_normal_(self.value_matrix)\n",
    "\n",
    "    self.output_matrix = nn.Parameter(torch.zeros(n_head, d_model, d_head))\n",
    "    nn.init.xavier_normal_(self.output_matrix)\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    query = einsum(data, self.query_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "    key = einsum(data, self.key_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "    value = einsum(data, self.value_matrix, \"batch seq_len d_model, n_head d_head d_model -> batch n_head seq_len d_head\")\n",
    "\n",
    "    attn_pre = einsum(query, key, \"batch n_head query_len d_head, batch n_head key_len d_head -> batch n_head query_len key_len\")\n",
    "    mask_idx = torch.triu_indices(self.seq_len, self.seq_len, offset=1)\n",
    "    attn_pre[..., mask_idx[0], mask_idx[1]] = float('-inf')\n",
    "    attn_pre /= self.d_head ** 0.5\n",
    "    attn = F.softmax(attn_pre, dim=-1)\n",
    "\n",
    "    output_pre = einsum(attn, value, \"batch n_head query_len key_len, batch n_head key_len d_head -> batch n_head key_len d_head\")\n",
    "    output = einsum(self.output_matrix, output_pre, \"n_head d_model d_head, batch n_head seq_len d_head -> batch seq_len d_model\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, d_model: int, d_mlp: int):\n",
    "    super().__init__()\n",
    "    self.MLP = nn.Sequential(nn.Linear(d_model, d_mlp),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(d_mlp, d_model))\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    return self.MLP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]):\n",
    "    mean: Float[Tensor, \"batch\"] = data.mean(dim=[1, 2], keepdim=True)\n",
    "    std: Float[Tensor, \"batch\"] = data.std(dim=[1, 2], keepdim=True)\n",
    "    return (data - mean) / (std + 1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembedding(nn.Module):\n",
    "  def __init__(self, d_model: int, seq_len: int):\n",
    "    super().__init__()\n",
    "    self.unembedding_matrix = nn.Parameter(torch.zeros(d_model, 256))\n",
    "    nn.init.xavier_normal_(self.unembedding_matrix)\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len 256\"]:\n",
    "    return einsum(self.unembedding_matrix, data, \"d_model d_vocab, batch seq_len d_model -> batch seq_len d_vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, n_heads: int, d_model: int, d_head: int, seq_len: int, d_mlp: int ):\n",
    "    super().__init__()\n",
    "    self.Attn = Attention(n_heads, d_model, d_head, seq_len)\n",
    "    self.MLP = MLP(d_model, d_mlp)\n",
    "    self.LayerNorm = LayerNorm()\n",
    "\n",
    "  def forward(self, data: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "    resid_attn = self.LayerNorm(data + self.Attn(data))\n",
    "    return self.LayerNorm(resid_attn + self.MLP(resid_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "  d_model: int\n",
    "  ctx_len: int\n",
    "  n_heads: int\n",
    "  d_head: int\n",
    "  d_mlp: int\n",
    "  n_layers: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "  def __init__(self, model_cfg: TransformerConfig):\n",
    "    super().__init__()\n",
    "    self.Embed = Embedding(model_cfg.d_model, model_cfg.ctx_len)   \n",
    "    self.Unembed = Unembedding(model_cfg.d_model, model_cfg.ctx_len)\n",
    "    self.Layers = nn.ModuleList([TransformerLayer(model_cfg.n_heads, model_cfg.d_model, model_cfg.d_head, model_cfg.ctx_len, model_cfg.d_mlp) for _ in range(model_cfg.n_layers)])\n",
    "\n",
    "  def forward(self, data: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len 256\"]:\n",
    "    x = self.Embed(data)\n",
    "    for tl in self.Layers: \n",
    "      x = tl(x)\n",
    "    return self.Unembed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(768, 512, 12, 64, 3072, 3)\n",
    "gpt2 = GPT2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(torch.randint(256, (1, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(gpt2.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "  for data in tqdm(train_dataloader, leave=False):\n",
    "    current_tok = data['current']\n",
    "    next_tok = data['next']\n",
    "\n",
    "    logits = gpt2(current_tok)\n",
    "    logits = rearrange(logits, \"batch ctx_len d_vocab -> batch d_vocab ctx_len\")\n",
    "    loss = loss_fn(logits, next_tok)\n",
    "    wandb.log({'loss': loss.item()})\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6vbhtt8j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▃█▅▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>4.97791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-brook-1</strong> at: <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/6vbhtt8j' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/6vbhtt8j</a><br/> View project at: <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241105_143341-6vbhtt8j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6vbhtt8j). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/femibello/Documents/projects/gpt2-rep/wandb/run-20241105_143424-o6ws123y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/o6ws123y' target=\"_blank\">radiant-disco-2</a></strong> to <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/o6ws123y' target=\"_blank\">https://wandb.ai/f3mi-the-university-of-texas-at-austin/gpt2-rep/runs/o6ws123y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3591e96a4eee4f07bcd2b96fef669caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3363ebfd854afcb1f794250943e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85772cdf2cf4bf1bdd403d60aca3931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-rep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[1;32m     10\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/autograd/__init__.py:346\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/gpt2-rep/.gpt2/lib/python3.12/site-packages/torch/autograd/graph.py:806\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"gpt2-rep\")\n",
    "for _ in tqdm(range(10)):\n",
    "  train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.Tensor([[ord(x) for x in \"\"\"No, no, it cannot be; and yet my heart\n",
    "\"\"\"]]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 39])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, no, it cannot be; and yet my heart\n",
      "arwht t pewbedoor t\n",
      "\n",
      "wl cieoti\n",
      ";is,e rsseeobat\n",
      "fMohap nttnhydueCe\n",
      " awt:ScheodaetVCs ostsizaihhr ca m tidch  t AwoldecczlclhnhtgsiietiIut ah.rtiimhcnd.ahtonssa  mcehttu\n",
      "saaSri nyicrlrmru mhoo rd\n",
      "ie Wbdu\n",
      "ifeshh   a ppataaduisopwfoahhyehm \n",
      "dmg.t\n",
      " ceervy ky\n",
      "?odhoe eerdAie:iokthamin etirewltirieytorcsn p\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Auto-regressive sampling with padding handling\n",
    "def sample(model, tokens, n_tokens, pad_token_id=0):\n",
    "    # Pad the initial tokens to 512 tokens\n",
    "    non_padded_len = tokens.size(1)\n",
    "    pad_length = 512 - tokens.size(1)\n",
    "    if pad_length > 0:\n",
    "        tokens = torch.cat([tokens, torch.full((tokens.size(0), pad_length), pad_token_id, dtype=tokens.dtype)], dim=1)\n",
    "    \n",
    "    current_length = non_padded_len\n",
    "    for _ in range(n_tokens):\n",
    "        logits = model(tokens)\n",
    "        logits = logits[0, current_length - 1]\n",
    "        # Sample the next token probabilistically\n",
    "        token = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "        # replace the last token with the sampled token\n",
    "        tokens[0, current_length] = token\n",
    "        current_length += 1\n",
    "    return tokens\n",
    "\n",
    "# Convert the token back to string\n",
    "def to_string(tokens):\n",
    "    return ''.join([chr(x) for x in tokens[0]])\n",
    "\n",
    "sampled_tokens = sample(gpt2, tokens, 300)\n",
    "print(to_string(sampled_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
